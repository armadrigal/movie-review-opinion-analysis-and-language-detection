{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf664ef",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import (Embedding, Bidirectional, LSTM, \n",
    "    Dense, Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Flatten, \n",
    "    Attention, MultiHeadAttention, Input, GRU, Concatenate)\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.metrics import Accuracy, Recall, Precision\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Model\n",
    "from nltk import FreqDist\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa32928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Data/IMDB Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cfecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['review_fr', 'sentiment']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba66f37",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7501a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_texts(texts):\n",
    "    clean_texts = []\n",
    "    for text in texts:\n",
    "        text = re.sub(r'[^a-zA-Záéíóúüñàâäéèêëîïôœùûç\\']', ' ', text)\n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        clean_texts.append(text)\n",
    "    return clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a00d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts):\n",
    "    tokenized_texts = []\n",
    "    for text in texts:\n",
    "        tokenized_texts.append(text.split())\n",
    "    return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    texts_without_stopwords = []\n",
    "    nltk.download(\"stopwords\")\n",
    "    stop_words = set(stopwords.words(\"portuguese\"))\n",
    "    for text in texts:\n",
    "        texts_without_stopwords.append([word for word in text if word not in stop_words])\n",
    "    return texts_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca0c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_texts(texts, vocabulary):\n",
    "    vectoriced_texts = []\n",
    "    for i, text in enumerate(texts):\n",
    "        vectoriced_text = []\n",
    "        for j, word in enumerate(text):\n",
    "            if word in vocabulary:\n",
    "                vectoriced_text.append(vocabulary[word]) \n",
    "            else:\n",
    "                vectoriced_text.append(0) \n",
    "        vectoriced_texts.append(vectoriced_text)\n",
    "    return vectoriced_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['review_it'].to_list()\n",
    "clean_texts = cleaning_texts(texts)\n",
    "tokenized_texts = tokenize_texts(clean_texts)\n",
    "tokenized_texts = remove_stopwords(tokenized_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c35557",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a25ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ' '.join(' '.join(text) for text in tokenized_texts)\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='black').generate(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7a393",
   "metadata": {},
   "source": [
    "## Top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40997aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for text in tokenized_texts:\n",
    "    words.extend(text)\n",
    "frequency = FreqDist(words)\n",
    "top_20 = frequency.most_common(20)\n",
    "words, freq = zip(*top_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032557b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(words, freq)\n",
    "plt.xlabel('Frecuencia')\n",
    "plt.ylabel('Palabras')\n",
    "plt.title('Top 20 Palabras en el Conjunto de Textos')\n",
    "plt.gca().invert_yaxis() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ed0b7",
   "metadata": {},
   "source": [
    "## Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c141d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec = Word2Vec(tokenized_texts, vector_size=50, window=10, min_count=10, workers=4)\n",
    "model_word2vec.train(tokenized_texts,total_examples=model_word2vec.corpus_count, epochs=20)\n",
    "model_word2vec.save(\"./API/Models/Word2Vec_it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dcad3a",
   "metadata": {},
   "source": [
    "## Vectorize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19302e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec = Word2Vec.load(\"./API/Models/Word2Vec_it\")\n",
    "vocabulary = model_word2vec.wv.key_to_index\n",
    "with open(\"./API/vocabulary/vocabulary_it\", \"wb\") as file:\n",
    "    pickle.dump(vocabulary, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 150\n",
    "max_words = len(vocabulary)\n",
    "\n",
    "vectorized_texts = vectorize_texts(tokenized_texts, vocabulary)\n",
    "X = pad_sequences(vectorized_texts, maxlen=maxlen)\n",
    "y = df['sentiment'].apply(lambda x : 1 if x == 'positive' else 0).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fdf8b",
   "metadata": {},
   "source": [
    "## Split dataset into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1083009",
   "metadata": {},
   "source": [
    "## Build embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd772f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((max_words, 50))\n",
    "for word, i in vocabulary.items():\n",
    "    if i < max_words:\n",
    "        if word in vocabulary:\n",
    "            embedding_matrix[i] = model_word2vec.wv[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384ee69",
   "metadata": {},
   "source": [
    "## Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a837003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_true, y_pred) \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(3, 5))\n",
    "    ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.8)\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='large')\n",
    " \n",
    "    plt.xlabel('Predictions', fontsize=12)\n",
    "    plt.ylabel('Actuals', fontsize=12)\n",
    "    plt.title('Confusion Matrix', fontsize=12)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
